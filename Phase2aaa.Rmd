---
title: "phase2aaa"
output: word_document
date: "2024-10-10"
---

```{r}
#load library
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(pROC)
library(lattice)
library(ggplot2)
library(forcats)  
library(dplyr)
```
```{r}
# read
train <- read_csv("C:/Users/nosta/Downloads/train.csv")
str(train)
summary(train)
```
```{r}
# Set seed for reproducibility
set.seed(123)

# Split the data into training and testing sets (80/20)
trainIndex <- createDataPartition(train$failure, p = 0.8, list = FALSE)
train_data <- train[trainIndex, ]
test_data <- train[-trainIndex, ]


```
```{r}
# Impute missing values using median imputation for numeric variables
preProcess_model <- preProcess(train_data, method = 'medianImpute')

# Apply the pre-processing model to train and test data
train_data <- predict(preProcess_model, newdata = train_data)
test_data <- predict(preProcess_model, newdata = test_data)

## Predict model with the random forest model
set.seed(123)
rf_model <- randomForest(failure ~ ., data = train_data)

# Plot variable importance
varImpPlot(rf_model)

# Predict on the test data
rf_pred <- predict(rf_model, test_data)

# Confusion matrix for random forest
confusionMatrix(rf_pred, test_data$failure)

```

```{r}

```{r}
# Remove rows with missing values
train_data <- na.omit(train_data)
test_data <- na.omit(test_data)

# Fit the random forest model
set.seed(123)
rf_model <- randomForest(failure ~ ., data = train_data)

# Plot variable importance
varImpPlot(rf_model)

# Predict on the test data
rf_pred <- predict(rf_model, test_data)

# Confusion matrix for random forest
confusionMatrix(rf_pred, test_data$failure)

```
```{r}
# Confusion matrix for random forest
rf_conf_matrix <- confusionMatrix(rf_pred, test_data$failure)

# Print the confusion matrix and key metrics
rf_conf_matrix

# Extract specific metrics
rf_accuracy <- rf_conf_matrix$overall['Accuracy']
rf_sensitivity <- rf_conf_matrix$byClass['Sensitivity']
rf_specificity <- rf_conf_matrix$byClass['Specificity']

# Print the metrics
rf_accuracy
rf_sensitivity
rf_specificity

```
```{r}
# Cross-validation for random forest
set.seed(123)
train_control <- trainControl(method = "cv", number = 10)

# Fit the random forest model with cross-validation
rf_cv <- train(failure ~ ., data = train_data, method = "rf", trControl = train_control)

# Print the results of cross-validation
print(rf_cv)

```

```{r}
# Predict model with logistic regression
log_model <- glm(failure ~ ., data = train_data, family = binomial)

# Summary of the logistic regression model
summary(log_model)

# Predict on the test data
log_pred <- predict(log_model, test_data, type = "response")
log_pred_class <- ifelse(log_pred > 0.5, "Yes", "No")

# Confusion matrix for logistic regression
confusionMatrix(as.factor(log_pred_class), as.factor(test_data$failure))

```
```{r}
# Evaluation for logistic regression
log_conf <- confusionMatrix(as.factor(log_pred_class), as.factor(test_data$failure))
log_conf$overall['Accuracy']  # Accuracy
log_conf$byClass['Sensitivity']  # Sensitivity
log_conf$byClass['Specificity']  # Specificity

```
```{r}
# Logistic regression cross-validation
train_control <- trainControl(method = "cv", number = 10)
log_cv <- train(failure ~ ., data = train_data, method = "glm", family = binomial, trControl = train_control)
log_cv

```

```{r}
##Predict model with classification tree
tree_model <- rpart(failure ~ ., data = train_data, method = "class")

# Plot the tree
rpart.plot(tree_model)

# Predict on the test data
tree_pred <- predict(tree_model, test_data, type = "class")

# Confusion matrix for classification tree
confusionMatrix(tree_pred, test_data$failure)


```
```{r}

# Confusion matrix for classification tree
tree_conf <- confusionMatrix(tree_pred, test_data$failure)

# Print the confusion matrix and key metrics
tree_conf
# Extract accuracy, sensitivity, and specificity
tree_accuracy <- tree_conf$overall['Accuracy']
tree_sensitivity <- tree_conf$byClass['Sensitivity']
tree_specificity <- tree_conf$byClass['Specificity']

# Print the key metrics
print(tree_accuracy)
print(tree_sensitivity)
print(tree_specificity)

```
```{r}
# Cross-validation for classification tree
set.seed(123)
tree_cv <- train(failure ~ ., data = train_data, method = "rpart", trControl = train_control)
tree_cv

```

```{r}
###comparison of models

# Logistic Regression Evaluation
log_accuracy <- log_conf$overall['Accuracy']
log_sensitivity <- log_conf$byClass['Sensitivity']
log_specificity <- log_conf$byClass['Specificity']

# Classification Tree Evaluation
tree_accuracy <- tree_conf$overall['Accuracy']
tree_sensitivity <- tree_conf$byClass['Sensitivity']
tree_specificity <- tree_conf$byClass['Specificity']

# Random Forest Evaluation
rf_accuracy <- rf_conf_matrix$overall['Accuracy']
rf_sensitivity <- rf_conf_matrix$byClass['Sensitivity']
rf_specificity <- rf_conf_matrix$byClass['Specificity']

# Create a summary table of model performances
performance_summary <- data.frame(
  Model = c("Logistic Regression", "Classification Tree", "Random Forest"),
  Accuracy = c(log_accuracy, tree_accuracy, rf_accuracy),
  Sensitivity = c(log_sensitivity, tree_sensitivity, rf_sensitivity),
  Specificity = c(log_specificity, tree_specificity, rf_specificity)
)

# Print the summary table
print(performance_summary)

```
```{r}

### Summary:
# **Training/Testing Split**: We split the data into training (80%) and testing (20%).
#**Multiple Models**: We built logistic regression, classification tree, and random forest models.
#**Cross-Validation**: We applied k-fold cross-validation to evaluate model robustness.
#**Evaluation Metrics**: Accuracy, sensitivity, specificity, and Kappa were used to evaluate model performance.

```

